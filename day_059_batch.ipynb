{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_059_batch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/100DayOfMLCode/blob/master/day_059_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jkp9iCabcFXS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Refactoring to add batching and feature-creation"
      ]
    },
    {
      "metadata": {
        "id": "_SuDl7BlcajY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "we continue reading the same small dataset, but refactor our ML pipeline in two small, but significant, ways:\n",
        "\n",
        "1.  Refactor the input to read data in batches.\n",
        "2. Refactor the feature creation so that it is not one-to-one with inputs. The Pandas function in the previous notebook also batched, only after it had read the whole data into memory -- on a large dataset, this won't be an option.\n"
      ]
    },
    {
      "metadata": {
        "id": "LgVfqZO3cshf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MvifrJ7Dc2Xw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Refactor the input\n",
        "\n",
        "Read data created, but this time make it more general and performant. Instead of using Pandas, we will use TensorFlow's Dataset API."
      ]
    },
    {
      "metadata": {
        "id": "szwO3mN0dKYg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csv_columns = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
        "label_column = 'fare_amount'\n",
        "defaults = [[0.0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]\n",
        "\n",
        "def read_dataset(filename, mode, batch_size = 512):\n",
        "  def _input_fn():\n",
        "    def decode_csv(value_column):\n",
        "      columns = tf.decode_csv(value_column, record_defaults = defaults)\n",
        "      features = dict(zip(csv_columns, columns))\n",
        "      label = features.pop(label_column)\n",
        "      return features, label\n",
        "    \n",
        "    # Create list of files that match pattern\n",
        "    file_list = tf.gfile.Glob(filename)\n",
        "    \n",
        "    # Create dataset from file list\n",
        "    dataset = tf.data.TextLineDataset(file_list).map(decode_csv)\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      num_epochs = None # indefinitely\n",
        "      dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "    else:\n",
        "      num_epochs = 1 # end-of-input after this\n",
        "    \n",
        "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "    return dataset.make_one_shot_iterator().get_next()\n",
        "  return _input_fn\n",
        "\n",
        "\n",
        "def get_train():\n",
        "  return read_dataset('./taxi-train.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "def get_valid():\n",
        "  return read_dataset('./taxi-valid.csv', mode = tf.estimator.ModeKeys.EVAL)\n",
        "\n",
        "def get_test():\n",
        "  return read_dataset('./taxi-test.csv', mode = tf.estimator.ModeKeys.EVAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8bcRU600gyR3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Refactoe the way features are created.\n",
        "\n",
        "For now, pass these through (same as previous). However, refactoring this way will enable us to break the one-to-one relationship between inputs and features."
      ]
    },
    {
      "metadata": {
        "id": "jqo_nsj6hHAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_columns = [\n",
        "    tf.feature_column.numeric_column('pickuplon'),\n",
        "    tf.feature_column.numeric_column('pickuplat'),\n",
        "    tf.feature_column.numeric_column('dropofflat'),\n",
        "    tf.feature_column.numeric_column('dropofflon'),\n",
        "    tf.feature_column.numeric_column('passengers'),\n",
        "]\n",
        "\n",
        "def add_more_features(feats):\n",
        "  # Nothing to add\n",
        "  return feats\n",
        "\n",
        "feature_cols = add_more_features(input_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ySVISysPht6g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create and train the model\n",
        "\n",
        "Note that we train for num_steps * batch_size example."
      ]
    },
    {
      "metadata": {
        "id": "M-F6yGFlh3Mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "8cc1098f-873f-45e7-b669-65b8a3f9bfea"
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "outdir = 'taxi_trained'\n",
        "shutil.rmtree(outdir, ignore_errors = True)\n",
        "model = tf.estimator.LinearRegressor(\n",
        "  feature_columns = feature_cols, model_dir = outdir)\n",
        "\n",
        "model.train(input_fn = get_train(), steps = 100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'taxi_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc736baacf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 112954.91, step = 1\n",
            "INFO:tensorflow:Saving checkpoints for 100 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 41582.848.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressor at 0x7fc736baaf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}