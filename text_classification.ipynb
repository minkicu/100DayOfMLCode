{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/100DayOfMLCode/blob/master/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BsiupQ2jq9g",
        "colab_type": "text"
      },
      "source": [
        "<h1> Text Classification using TensorFlow/Keras on Cloud ML Engine </h1>\n",
        "\n",
        "This notebook illustrates:\n",
        "<ol>\n",
        "<li> Creating datasets for Machine Learning using BigQuery\n",
        "<li> Creating a text classification model using the Estimator API with a Keras model\n",
        "<li> Training on Cloud ML Engine\n",
        "<li> Deploying the model\n",
        "<li> Predicting with model\n",
        "<li> Rerun with pre-trained embedding\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKqDiQrBjq9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change these to try this notebook out\n",
        "BUCKET = 'qwiklabs-gcp-ac18823b97c7719f'\n",
        "PROJECT = 'qwiklabs-gcp-ac18823b97c7719f'\n",
        "REGION = 'us-central1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct7LqDA7jq9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['TFVERSION'] = '1.8'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awg2PkiAjq9n",
        "colab_type": "code",
        "colab": {},
        "outputId": "dfabf601-a0c7-4813-ac5a-4b610889afc3"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD9xSyG9jq9s",
        "colab_type": "text"
      },
      "source": [
        "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
        "\n",
        "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8UQNE5Djq9t",
        "colab_type": "text"
      },
      "source": [
        "### Creating Dataset from BigQuery \n",
        "\n",
        "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
        "\n",
        "Here is a sample of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRx3MEB0jq9u",
        "colab_type": "code",
        "colab": {},
        "outputId": "f4ddf7ad-1cf8-4da6-9cb1-1eabb7a502a6"
      },
      "source": [
        "import google.datalab.bigquery as bq\n",
        "query=\"\"\"\n",
        "SELECT\n",
        "  url, title, score\n",
        "FROM\n",
        "  `bigquery-public-data.hacker_news.stories`\n",
        "WHERE\n",
        "  LENGTH(title) > 10\n",
        "  AND score > 10\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "df = bq.Query(query).execute().result().to_dataframe()\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://www.dumpert.nl/mediabase/6560049/3eb18e...</td>\n",
              "      <td>Calling the NSA: \"I accidentally deleted an e-...</td>\n",
              "      <td>258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>Show HN: Panda now with Product Hunt and more</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "      <td>Skype is down</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://blog.liip.ch/archive/2013/10/28/hhvm-an...</td>\n",
              "      <td>Amazing performance with HHVM and PHP with a S...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://www.gamedev.net/page/resources/_/techni...</td>\n",
              "      <td>A Journey Through the CPU Pipeline</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td></td>\n",
              "      <td>Ask HN: Has Sortfolio worked for you as a cont...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td></td>\n",
              "      <td>Offer HN: Free SEO For Your Startup/Company</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td></td>\n",
              "      <td>WARN HN: Gmail account fishing</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>http://jfarcand.wordpress.com/2011/02/25/atmos...</td>\n",
              "      <td>Atmosphere Framework 0.7 released: GWT, Wicket...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td></td>\n",
              "      <td>Ask HN: Which mobile advertising platforms do ...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 url  \\\n",
              "0  http://www.dumpert.nl/mediabase/6560049/3eb18e...   \n",
              "1                                                      \n",
              "2                                               None   \n",
              "3  http://blog.liip.ch/archive/2013/10/28/hhvm-an...   \n",
              "4  http://www.gamedev.net/page/resources/_/techni...   \n",
              "5                                                      \n",
              "6                                                      \n",
              "7                                                      \n",
              "8  http://jfarcand.wordpress.com/2011/02/25/atmos...   \n",
              "9                                                      \n",
              "\n",
              "                                               title  score  \n",
              "0  Calling the NSA: \"I accidentally deleted an e-...    258  \n",
              "1      Show HN: Panda now with Product Hunt and more     11  \n",
              "2                                      Skype is down     11  \n",
              "3  Amazing performance with HHVM and PHP with a S...     11  \n",
              "4                 A Journey Through the CPU Pipeline     11  \n",
              "5  Ask HN: Has Sortfolio worked for you as a cont...     11  \n",
              "6        Offer HN: Free SEO For Your Startup/Company     11  \n",
              "7                     WARN HN: Gmail account fishing     11  \n",
              "8  Atmosphere Framework 0.7 released: GWT, Wicket...     11  \n",
              "9  Ask HN: Which mobile advertising platforms do ...     11  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbrdGoQgjq9x",
        "colab_type": "text"
      },
      "source": [
        "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM-_Sl3Qjq9x",
        "colab_type": "code",
        "colab": {},
        "outputId": "02655234-72c4-4a39-c1f6-f0eeae01e11e"
      },
      "source": [
        "query=\"\"\"\n",
        "SELECT\n",
        "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
        "  COUNT(title) AS num_articles\n",
        "FROM\n",
        "  `bigquery-public-data.hacker_news.stories`\n",
        "WHERE\n",
        "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
        "  AND LENGTH(title) > 10\n",
        "GROUP BY\n",
        "  source\n",
        "ORDER BY num_articles DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "df = bq.Query(query).execute().result().to_dataframe()\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>num_articles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>blogspot</td>\n",
              "      <td>41386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>github</td>\n",
              "      <td>36525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>techcrunch</td>\n",
              "      <td>30891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>youtube</td>\n",
              "      <td>30848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nytimes</td>\n",
              "      <td>28787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>medium</td>\n",
              "      <td>18422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>google</td>\n",
              "      <td>18235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wordpress</td>\n",
              "      <td>17667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>arstechnica</td>\n",
              "      <td>13749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wired</td>\n",
              "      <td>12841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        source  num_articles\n",
              "0     blogspot         41386\n",
              "1       github         36525\n",
              "2   techcrunch         30891\n",
              "3      youtube         30848\n",
              "4      nytimes         28787\n",
              "5       medium         18422\n",
              "6       google         18235\n",
              "7    wordpress         17667\n",
              "8  arstechnica         13749\n",
              "9        wired         12841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFtSfBSjq90",
        "colab_type": "text"
      },
      "source": [
        "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-WrV81Cjq91",
        "colab_type": "code",
        "colab": {},
        "outputId": "594a2599-3bd8-460c-bfa6-c4bd02580e47"
      },
      "source": [
        "query=\"\"\"\n",
        "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
        "  (SELECT\n",
        "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
        "    title\n",
        "  FROM\n",
        "    `bigquery-public-data.hacker_news.stories`\n",
        "  WHERE\n",
        "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
        "    AND LENGTH(title) > 10\n",
        "  )\n",
        "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
        "\"\"\"\n",
        "\n",
        "df = bq.Query(query + \" LIMIT 10\").execute().result().to_dataframe()\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>github</td>\n",
              "      <td>php bdd is now  nice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>github</td>\n",
              "      <td>mpv video player 0.2 release</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>github</td>\n",
              "      <td>show hn  re-thinking the business card with dr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>github</td>\n",
              "      <td>update css js from chrome developer tool</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>github</td>\n",
              "      <td>simple way to start development with flask usi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   source                                              title\n",
              "0  github                              php bdd is now  nice \n",
              "1  github                       mpv video player 0.2 release\n",
              "2  github  show hn  re-thinking the business card with dr...\n",
              "3  github           update css js from chrome developer tool\n",
              "4  github  simple way to start development with flask usi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LrrbO3hjq94",
        "colab_type": "text"
      },
      "source": [
        "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
        "\n",
        "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYozqcstjq94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindf = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) > 0\").execute().result().to_dataframe()\n",
        "evaldf  = bq.Query(query + \" AND MOD(ABS(FARM_FINGERPRINT(title)),4) = 0\").execute().result().to_dataframe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gDjHP6cjq97",
        "colab_type": "text"
      },
      "source": [
        "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
        "\n",
        "We can also see that within each dataset, the classes are roughly balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuzQwDAfjq98",
        "colab_type": "code",
        "colab": {},
        "outputId": "d21500d4-b5ca-4cd3-e977-d585bc565545"
      },
      "source": [
        "traindf['source'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "github        27445\n",
              "techcrunch    23131\n",
              "nytimes       21586\n",
              "Name: source, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuLj0BANjq9_",
        "colab_type": "code",
        "colab": {},
        "outputId": "2017b027-7293-401d-9e1e-c53341d91b4d"
      },
      "source": [
        "evaldf['source'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "github        9080\n",
              "techcrunch    7760\n",
              "nytimes       7201\n",
              "Name: source, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7JUHQIujq-C",
        "colab_type": "text"
      },
      "source": [
        "Finally we will save our data, which is currently in-memory, to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxtGWDPyjq-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, shutil\n",
        "DATADIR='data/txtcls'\n",
        "shutil.rmtree(DATADIR, ignore_errors=True)\n",
        "os.makedirs(DATADIR)\n",
        "traindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n",
        "evaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XinsD8lSjq-F",
        "colab_type": "code",
        "colab": {},
        "outputId": "4ac16565-7520-4afc-ac41-632f7b346f66"
      },
      "source": [
        "!head -3 data/txtcls/train.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "github\tfeminist-software-foundation complains about reddit on a github pull request\r\n",
            "github\texpose sps as web services on the fly.\r\n",
            "github\tshow hn  linuxexplorer\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzGoipm_jq-I",
        "colab_type": "code",
        "colab": {},
        "outputId": "78dc1eff-1526-4231-81e5-4786e41ce1c5"
      },
      "source": [
        "!wc -l data/txtcls/*.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  24041 data/txtcls/eval.tsv\r\n",
            "  72162 data/txtcls/train.tsv\r\n",
            "  96203 total\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL12gvbNjq-L",
        "colab_type": "text"
      },
      "source": [
        "### TensorFlow/Keras Code\n",
        "\n",
        "Please explore the code in this <a href=\"txtclsmodel/trainer\">directory</a>: `model.py` contains the TensorFlow model and `task.py` parses command line arguments and launches off the training job.\n",
        "\n",
        "There are some TODOs in the `model.py`, **make sure to complete the TODOs before proceeding!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2yhazrrjq-L",
        "colab_type": "text"
      },
      "source": [
        "### Run Locally\n",
        "Let's make sure the code compiles by running locally for a fraction of an epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpdn-tehjq-M",
        "colab_type": "code",
        "colab": {},
        "outputId": "804a88cc-b4f0-424a-aa1d-40168ac02292"
      },
      "source": [
        "%%bash\n",
        "## Make sure we have the latest version of Google Cloud Storage package\n",
        "pip install --upgrade google-cloud-storage\n",
        "rm -rf txtcls_trained\n",
        "gcloud ml-engine local train \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=${PWD}/txtclsmodel/trainer \\\n",
        "   -- \\\n",
        "   --output_dir=${PWD}/txtcls_trained \\\n",
        "   --train_data_path=${PWD}/data/txtcls/train.tsv \\\n",
        "   --eval_data_path=${PWD}/data/txtcls/eval.tsv \\\n",
        "   --num_epochs=0.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-storage\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/aa/048f5b3950f78c9e6afdb05e3667abb7a7ca4463bfde002257acd1874c3f/google_cloud_storage-1.15.0-py2.py3-none-any.whl (64kB)\n",
            "Collecting google-cloud-core<0.30dev,>=0.29.0 (from google-cloud-storage)\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/f2/3c225e7a69cb27d283b68bff867722bd066bc1858611180197f711815ea5/google_cloud_core-0.29.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: google-resumable-media>=0.3.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-cloud-storage) (0.3.2)\n",
            "Collecting google-api-core<2.0.0dev,>=1.6.0 (from google-cloud-storage)\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/3d/328de10db1b3ec788faa65419727b223b720e9812c9c8660a390b3d56ee9/google_api_core-1.10.0-py2.py3-none-any.whl (65kB)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-resumable-media>=0.3.1->google-cloud-storage) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (2018.4)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (3.6.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (40.2.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=0.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (1.6.2)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos!=1.5.4,<2.0dev,>=1.5.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (1.5.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/envs/py3env/lib/python3.5/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (2018.8.24)\n",
            "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (3.4.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from rsa>=3.1.4->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0dev,>=1.6.0->google-cloud-storage) (0.4.4)\n",
            "Installing collected packages: google-api-core, google-cloud-core, google-cloud-storage\n",
            "  Found existing installation: google-api-core 0.1.4\n",
            "    Uninstalling google-api-core-0.1.4:\n",
            "      Successfully uninstalled google-api-core-0.1.4\n",
            "  Found existing installation: google-cloud-core 0.28.1\n",
            "    Uninstalling google-cloud-core-0.28.1:\n",
            "      Successfully uninstalled google-cloud-core-0.28.1\n",
            "Successfully installed google-api-core-1.10.0 google-cloud-core-0.29.1 google-cloud-storage-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "google-cloud-monitoring 0.28.0 has requirement google-api-core<0.2.0dev,>=0.1.1, but you'll have google-api-core 1.10.0 which is incompatible.\n",
            "google-cloud-monitoring 0.28.0 has requirement google-cloud-core<0.29dev,>=0.28.0, but you'll have google-cloud-core 0.29.1 which is incompatible.\n",
            "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {}, 'environment': 'cloud', 'task': {}, 'job': {'args': ['--output_dir=/content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained', '--train_data_path=/content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/data/txtcls/train.tsv', '--eval_data_path=/content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/data/txtcls/eval.tsv', '--num_epochs=0.1'], 'job_name': 'trainer.task'}}\n",
            "INFO:tensorflow:Using the Keras model provided.\n",
            "INFO:tensorflow:Using config: {'_global_id_in_cluster': 0, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_session_config': None, '_keep_checkpoint_max': 5, '_train_distribute': None, '_service': None, '_is_chief': True, '_model_dir': '/content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained', '_save_checkpoints_secs': None, '_save_checkpoints_steps': 500, '_log_step_count_steps': 100, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5e9eca2c18>, '_keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', '_task_type': 'worker', '_master': '', '_num_worker_replicas': 1, '_task_id': 0}\n",
            "2019-05-06 15:02:02.346179: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 10 secs (eval_spec.throttle_secs) or training is finished.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/keras_model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 1 into /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt.\n",
            "INFO:tensorflow:step = 1, loss = 2.1976402\n",
            "INFO:tensorflow:Saving checkpoints for 31 into /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.9834066.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-05-06-15:02:18\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-31\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-05-06-15:02:24\n",
            "INFO:tensorflow:Saving dict for global step 31: acc = 0.3723404, global_step = 31, loss = 1.273344\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Restoring parameters from /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-31\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: b\"/content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/export/exporter/temp-b'1557154944'/saved_model.pb\"\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-31\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 32 into /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt.\n",
            "INFO:tensorflow:step = 32, loss = 1.6389074\n",
            "INFO:tensorflow:Saving checkpoints for 57 into /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.9317486.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-05-06-15:02:34\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-57\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-05-06-15:02:40\n",
            "INFO:tensorflow:Saving dict for global step 57: acc = 0.3723404, global_step = 57, loss = 1.4421103\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
            "INFO:tensorflow:Restoring parameters from /content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-57\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: b\"/content/datalab/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/export/exporter/temp-b'1557154960'/saved_model.pb\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGMZEcL_jq-R",
        "colab_type": "text"
      },
      "source": [
        "### Train on the Cloud\n",
        "\n",
        "Let's first copy our training data to the cloud:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kYF_YUEjq-R",
        "colab_type": "code",
        "colab": {},
        "outputId": "97b2c3f7-f4ae-4bbe-f693-84eb3dbaf1d5"
      },
      "source": [
        "%%bash\n",
        "gsutil cp data/txtcls/*.tsv gs://${BUCKET}/txtcls/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://data/txtcls/eval.tsv [Content-Type=text/tab-separated-values]...\n",
            "/ [0 files][    0.0 B/  1.4 MiB]                                                \r/ [1 files][  1.4 MiB/  1.4 MiB]                                                \rCopying file://data/txtcls/train.tsv [Content-Type=text/tab-separated-values]...\n",
            "/ [1 files][  1.4 MiB/  5.4 MiB]                                                \r/ [2 files][  5.4 MiB/  5.4 MiB]                                                \r-\r\n",
            "Operation completed over 2 objects/5.4 MiB.                                      \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uphGNDpjq-V",
        "colab_type": "code",
        "colab": {},
        "outputId": "6825326b-08a4-4340-edf1-24656df19531"
      },
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/txtcls/trained_fromscratch\n",
        "JOBNAME=txtcls_$(date -u +%y%m%d_%H%M%S)\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        " --region=$REGION \\\n",
        " --module-name=trainer.task \\\n",
        " --package-path=${PWD}/txtclsmodel/trainer \\\n",
        " --job-dir=$OUTDIR \\\n",
        " --scale-tier=BASIC_GPU \\\n",
        " --runtime-version=$TFVERSION \\\n",
        " -- \\\n",
        " --output_dir=$OUTDIR \\\n",
        " --train_data_path=gs://${BUCKET}/txtcls/train.tsv \\\n",
        " --eval_data_path=gs://${BUCKET}/txtcls/eval.tsv \\\n",
        " --num_epochs=5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: txtcls_190506_150315\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "CommandException: 1 files/objects could not be removed.\n",
            "Job [txtcls_190506_150315] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs describe txtcls_190506_150315\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ml-engine jobs stream-logs txtcls_190506_150315\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DdNokp2jq-Z",
        "colab_type": "text"
      },
      "source": [
        "### Monitor training with TensorBoard\n",
        "If tensorboard appears blank try refreshing after 10 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0hqVJj9jq-a",
        "colab_type": "code",
        "colab": {},
        "outputId": "6c54e3e7-300c-4aed-ca16-cd10a5bbe9eb"
      },
      "source": [
        "from google.datalab.ml import TensorBoard\n",
        "TensorBoard().start('gs://{}/txtcls/trained_fromscratch'.format(BUCKET))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p>TensorBoard was started successfully with pid 4524. Click <a href=\"/_proxy/33027/\" target=\"_blank\">here</a> to access it.</p>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76AG1s3kjq-c",
        "colab_type": "code",
        "colab": {},
        "outputId": "be34c40a-50d5-43b0-8d8a-df4bd239c7e9"
      },
      "source": [
        "for pid in TensorBoard.list()['pid']:\n",
        "  TensorBoard().stop(pid)\n",
        "  print('Stopped TensorBoard with pid {}'.format(pid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stopped TensorBoard with pid 4524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7c5rSqZjq-e",
        "colab_type": "text"
      },
      "source": [
        "### Results\n",
        "What accuracy did you get?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJSBl3hfjq-f",
        "colab_type": "text"
      },
      "source": [
        "### Deploy trained model \n",
        "\n",
        "Once your training completes you will see your exported models in the output directory specified in Google Cloud Storage. \n",
        "\n",
        "You should see one model for each training checkpoint (default is every 1000 steps)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWhnE9u3jq-g",
        "colab_type": "code",
        "colab": {},
        "outputId": "9da478d5-1f8f-46ad-8865-f7d172f3649a"
      },
      "source": [
        "%%bash\n",
        "gsutil ls gs://${BUCKET}/txtcls/trained_fromscratch/export/exporter/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/\n",
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/1557155168/\n",
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/1557155191/\n",
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/1557155212/\n",
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/1557155234/\n",
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/1557155255/\n",
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/1557155277/\n",
            "gs://qwiklabs-gcp-ac18823b97c7719f/txtcls/trained_fromscratch/export/exporter/1557155295/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9tNvVXEjq-j",
        "colab_type": "text"
      },
      "source": [
        "We will take the last export and deploy it as a REST API using Google Cloud Machine Learning Engine "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX8L89krjq-k",
        "colab_type": "code",
        "colab": {},
        "outputId": "2b43137c-4321-418c-b374-8133eefe4f24"
      },
      "source": [
        "%%bash\n",
        "MODEL_NAME=\"txtcls\"\n",
        "MODEL_VERSION=\"v1_fromscratch\"\n",
        "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/txtcls/trained_fromscratch/export/exporter/ | tail -1)\n",
        "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME} --quiet\n",
        "#gcloud ml-engine models delete ${MODEL_NAME}\n",
        "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
        "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created ml engine model [projects/qwiklabs-gcp-ac18823b97c7719f/models/txtcls].\n",
            "Creating version (this might take a few minutes)......\n",
            "...............................................................................................................................................................................................................................................................................................................................................................................................done.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j22Ee9HOjq-o",
        "colab_type": "text"
      },
      "source": [
        "### Get Predictions\n",
        "\n",
        "Here are some actual hacker news headlines gathered from July 2018. These titles were not part of the training or evaluation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yk0AtP2jq-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "techcrunch=[\n",
        "  'Uber shuts down self-driving trucks unit',\n",
        "  'Grover raises €37M Series A to offer latest tech products as a subscription',\n",
        "  'Tech companies can now bid on the Pentagon’s $10B cloud contract'\n",
        "]\n",
        "nytimes=[\n",
        "  '‘Lopping,’ ‘Tips’ and the ‘Z-List’: Bias Lawsuit Explores Harvard’s Admissions',\n",
        "  'A $3B Plan to Turn Hoover Dam into a Giant Battery',\n",
        "  'A MeToo Reckoning in China’s Workplace Amid Wave of Accusations'\n",
        "]\n",
        "github=[\n",
        "  'Show HN: Moon – 3kb JavaScript UI compiler',\n",
        "  'Show HN: Hello, a CLI tool for managing social media',\n",
        "  'Firefox Nightly added support for time-travel debugging'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw6aofDIjq-q",
        "colab_type": "text"
      },
      "source": [
        "Our serving input function expects the already tokenized representations of the headlines, so we do that pre-processing in the code before calling the REST API.\n",
        "\n",
        "Note: Ideally we would do these transformation in the tensorflow graph directly instead of relying on separate client pre-processing code (see: [training-serving skew](https://developers.google.com/machine-learning/guides/rules-of-ml/#training_serving_skew)), howevever the pre-processing functions we're using are python functions so cannot be embedded in a tensorflow graph. \n",
        "\n",
        "See the <a href=\"../text_classification_native.ipynb\">text_classification_native</a> notebook for a solution to this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNfSdwdujq-r",
        "colab_type": "code",
        "colab": {},
        "outputId": "dda66757-2229-4196-ade2-9e36f395dfbb"
      },
      "source": [
        "import pickle\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from googleapiclient import discovery\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import json\n",
        "\n",
        "requests = techcrunch+nytimes+github\n",
        "\n",
        "# Tokenize and pad sentences using same mapping used in the deployed model\n",
        "tokenizer = pickle.load( open( \"txtclsmodel/tokenizer.pickled\", \"rb\" ) )\n",
        "\n",
        "requests_tokenized = tokenizer.texts_to_sequences(requests)\n",
        "requests_tokenized = sequence.pad_sequences(requests_tokenized,maxlen=50)\n",
        "\n",
        "# JSON format the requests\n",
        "request_data = {'instances':requests_tokenized.tolist()}\n",
        "\n",
        "# Authenticate and call CMLE prediction API \n",
        "credentials = GoogleCredentials.get_application_default()\n",
        "api = discovery.build('ml', 'v1', credentials=credentials,\n",
        "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
        "\n",
        "parent = 'projects/%s/models/%s' % (PROJECT, 'txtcls') #version is not specified so uses default\n",
        "response = api.projects().predict(body=request_data, name=parent).execute()\n",
        "\n",
        "# Format and print response\n",
        "for i in range(len(requests)):\n",
        "  print('\\n{}'.format(requests[i]))\n",
        "  print(' github    : {}'.format(response['predictions'][i]['dense_1'][0]))\n",
        "  print(' nytimes   : {}'.format(response['predictions'][i]['dense_1'][1]))\n",
        "  print(' techcrunch: {}'.format(response['predictions'][i]['dense_1'][2]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Uber shuts down self-driving trucks unit\n",
            " github    : 0.00026765494840219617\n",
            " nytimes   : 0.11095231026411057\n",
            " techcrunch: 0.8887799978256226\n",
            "\n",
            "Grover raises €37M Series A to offer latest tech products as a subscription\n",
            " github    : 9.00617305887863e-06\n",
            " nytimes   : 0.0863744243979454\n",
            " techcrunch: 0.9136165380477905\n",
            "\n",
            "Tech companies can now bid on the Pentagon’s $10B cloud contract\n",
            " github    : 0.0003065938944928348\n",
            " nytimes   : 0.32079777121543884\n",
            " techcrunch: 0.6788956522941589\n",
            "\n",
            "‘Lopping,’ ‘Tips’ and the ‘Z-List’: Bias Lawsuit Explores Harvard’s Admissions\n",
            " github    : 0.006772536318749189\n",
            " nytimes   : 0.6514438986778259\n",
            " techcrunch: 0.3417835533618927\n",
            "\n",
            "A $3B Plan to Turn Hoover Dam into a Giant Battery\n",
            " github    : 0.004488898906856775\n",
            " nytimes   : 0.9875668287277222\n",
            " techcrunch: 0.007944222539663315\n",
            "\n",
            "A MeToo Reckoning in China’s Workplace Amid Wave of Accusations\n",
            " github    : 0.00045296267489902675\n",
            " nytimes   : 0.9967542290687561\n",
            " techcrunch: 0.002792845480144024\n",
            "\n",
            "Show HN: Moon – 3kb JavaScript UI compiler\n",
            " github    : 0.9999504089355469\n",
            " nytimes   : 1.6731472669562208e-06\n",
            " techcrunch: 4.7861423809081316e-05\n",
            "\n",
            "Show HN: Hello, a CLI tool for managing social media\n",
            " github    : 0.9998893737792969\n",
            " nytimes   : 3.348704353811627e-07\n",
            " techcrunch: 0.00011025866842828691\n",
            "\n",
            "Firefox Nightly added support for time-travel debugging\n",
            " github    : 0.1278354525566101\n",
            " nytimes   : 0.12993191182613373\n",
            " techcrunch: 0.742232620716095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J17TynTzjq-t",
        "colab_type": "text"
      },
      "source": [
        "How many of your predictions were correct?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRTsYJRSjq-u",
        "colab_type": "text"
      },
      "source": [
        "### Rerun with Pre-trained Embedding\n",
        "\n",
        "In the previous model we trained our word embedding from scratch. Often times we get better performance and/or converge faster by leveraging a pre-trained embedding. This is a similar concept to transfer learning during image classification.\n",
        "\n",
        "We will use the popular GloVe embedding which is trained on Wikipedia as well as various news sources like the New York Times.\n",
        "\n",
        "You can read more about Glove at the project homepage: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "You can download the embedding files directly from the stanford.edu site, but we've rehosted it in a GCS bucket for faster download speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9-MvlhXjq-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/text_classification/glove.6B.200d.txt gs://$BUCKET/txtcls/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnr_Cphijq-x",
        "colab_type": "text"
      },
      "source": [
        "Once the embedding is downloaded re-run your cloud training job with the added command line argument: \n",
        "\n",
        "` --embedding_path=gs://${BUCKET}/txtcls/glove.6B.200d.txt`\n",
        "\n",
        "Be sure to change your OUTDIR so it doesn't overwrite the previous model.\n",
        "\n",
        "While the final accuracy may not change significantly, you should notice the model is able to converge to it much more quickly because it no longer has to learn an embedding from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVw5U26jq-x",
        "colab_type": "text"
      },
      "source": [
        "#### References\n",
        "- This implementation is based on code from: https://github.com/google/eng-edu/tree/master/ml/guides/text_classification.\n",
        "- See the full text classification tutorial at: https://developers.google.com/machine-learning/guides/text-classification/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqdtoLvojq-y",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    }
  ]
}