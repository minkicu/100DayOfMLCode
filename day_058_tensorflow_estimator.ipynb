{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_058_tensorflow_estimator.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/100DayOfMLCode/blob/master/day_058_tensorflow_estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "471EOLR-aMzZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## tf.estimator\n",
        "\n",
        "we will create a machine learning model using tf.estimator and evaluate its performance. The dataset is rather small (7700 samples), so we can do it all in-memory. We will also simply pass the raw data in as-is"
      ]
    },
    {
      "metadata": {
        "id": "pKcP_Yqka3PN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1125
        },
        "outputId": "97915805-6081-4992-eaec-bf1ff0e33db1"
      },
      "cell_type": "code",
      "source": [
        "!pip install datalab"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datalab in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: google-cloud>=0.30.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.34.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.2 in /usr/local/lib/python3.6/dist-packages (from datalab) (1.6.7)\n",
            "Requirement already satisfied: seaborn>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.22 in /usr/local/lib/python3.6/dist-packages (from datalab) (1.22)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (2.6.0)\n",
            "Requirement already satisfied: configparser>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (3.7.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.2 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.0.3)\n",
            "Requirement already satisfied: six==1.10.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (1.10.0)\n",
            "Requirement already satisfied: oauth2client>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (4.1.3)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from datalab) (2.18.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.2 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (2.5.3)\n",
            "Requirement already satisfied: scikit-image>=0.13.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.13.1)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python3.6/dist-packages (from datalab) (3.13)\n",
            "Requirement already satisfied: plotly>=1.12.5 in /usr/local/lib/python3.6/dist-packages (from datalab) (3.6.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2015.4 in /usr/local/lib/python3.6/dist-packages (from datalab) (2018.9)\n",
            "Requirement already satisfied: ipykernel>=4.5.2 in /usr/local/lib/python3.6/dist-packages (from datalab) (4.6.1)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.22.0)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.16.0)\n",
            "Requirement already satisfied: httplib2>=0.10.3 in /usr/local/lib/python3.6/dist-packages (from datalab) (0.11.3)\n",
            "Requirement already satisfied: psutil>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from datalab) (5.4.8)\n",
            "Requirement already satisfied: pandas-profiling>=1.0.0a2 in /usr/local/lib/python3.6/dist-packages (from datalab) (1.4.1)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.2->datalab) (3.0.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.6/dist-packages (from google-auth-httplib2>=0.0.2->datalab) (1.4.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=2.2.0->datalab) (0.2.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=2.2.0->datalab) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=2.2.0->datalab) (0.4.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->datalab) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->datalab) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.9.1->datalab) (2019.3.9)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.2->datalab) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.2->datalab) (1.14.6)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.13.0->datalab) (2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.13.0->datalab) (1.0.2)\n",
            "Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.13.0->datalab) (4.1.1)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.13.0->datalab) (3.0.3)\n",
            "Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/dist-packages (from plotly>=1.12.5->datalab) (4.4.0)\n",
            "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/dist-packages (from plotly>=1.12.5->datalab) (4.4.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=1.12.5->datalab) (1.3.3)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->datalab) (5.1.3)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.2->datalab) (4.3.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.2->datalab) (5.2.4)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.2->datalab) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.2->datalab) (5.5.0)\n",
            "Requirement already satisfied: jinja2>=2.8 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling>=1.0.0a2->datalab) (2.10)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth->google-auth-httplib2>=0.0.2->datalab) (3.1.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=2.1.0->scikit-image>=0.13.0->datalab) (0.46)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.13.0->datalab) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.13.0->datalab) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.13.0->datalab) (2.3.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly>=1.12.5->datalab) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly>=1.12.5->datalab) (4.4.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.2->datalab) (17.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab) (4.6.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab) (40.9.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab) (1.0.15)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab) (2.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.8->pandas-profiling>=1.0.0a2->datalab) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel>=4.5.2->datalab) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel>=4.5.2->datalab) (0.1.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fOKnAW9bakem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "beb4eac5-170a-4690-e3b3-4a08fb5460ce"
      },
      "cell_type": "code",
      "source": [
        "import datalab.bigquery as bq\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EZCJMSdWbFi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Read data created in the previous chapter."
      ]
    },
    {
      "metadata": {
        "id": "3Rfp6FdRbGfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csv_columns = ['fare_amount','pickuplon','pickuplat','dropofflon','dropofflat','passengers','key']\n",
        "features = csv_columns[1:len(csv_columns)-1]\n",
        "label = csv_columns[0]\n",
        "\n",
        "df_train = pd.read_csv('./taxi-train.csv',header = None, names = csv_columns)\n",
        "df_valid = pd.read_csv('./taxi-valid.csv',header = None, names = csv_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RThZQaxqdM53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Input function to read from Pandas Dataframe into tf.constant"
      ]
    },
    {
      "metadata": {
        "id": "mg-I8tPcdQN9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_input_fn(df, num_epochs):\n",
        "  return tf.estimator.inputs.pandas_input_fn(\n",
        "    x = df,\n",
        "    y = df[label],\n",
        "    batch_size = 128,\n",
        "    num_epochs = num_epochs,\n",
        "    shuffle = True,\n",
        "    queue_capacity = 1000,\n",
        "    num_threads =1\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhURUDZAppBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tvUxP8od0P_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create feature columns for estimator"
      ]
    },
    {
      "metadata": {
        "id": "ykzHExNkd36O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_feature_cols():\n",
        "  input_columns = [tf.feature_column.numeric_column(k) for k in features]\n",
        "  return input_columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PT-YZtsfny0z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Linear Regression with tf.Estimator framework"
      ]
    },
    {
      "metadata": {
        "id": "l6-tndmFn2lD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "4328d5c3-eb10-4360-a4cf-a3614eeaf212"
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "outdir = 'taxi_trained'\n",
        "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
        "\n",
        "model = tf.estimator.LinearRegressor(\n",
        "      feature_columns = make_feature_cols(), model_dir = outdir)\n",
        "\n",
        "model.train(input_fn = make_input_fn(df_train, num_epochs = 10))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'taxi_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f01c5ab5128>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 21085.254, step = 1\n",
            "INFO:tensorflow:global_step/sec: 332.959\n",
            "INFO:tensorflow:loss = 5913.359, step = 101 (0.302 sec)\n",
            "INFO:tensorflow:global_step/sec: 347.032\n",
            "INFO:tensorflow:loss = 8118.034, step = 201 (0.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 359.571\n",
            "INFO:tensorflow:loss = 6653.8896, step = 301 (0.282 sec)\n",
            "INFO:tensorflow:global_step/sec: 351.286\n",
            "INFO:tensorflow:loss = 16479.723, step = 401 (0.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 357.397\n",
            "INFO:tensorflow:loss = 6521.8545, step = 501 (0.284 sec)\n",
            "INFO:tensorflow:global_step/sec: 375.799\n",
            "INFO:tensorflow:loss = 5476.468, step = 601 (0.264 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 608 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 135.8489.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressor at 0x7f01c5aa8ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "BOzYutu6qv9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluate on the validation data (we should defer using the test data to after we have selected a final model)."
      ]
    },
    {
      "metadata": {
        "id": "Wjfs1SjHqxQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "0689ac57-3620-4634-bd2c-32361e1e0be4"
      },
      "cell_type": "code",
      "source": [
        "def print_rmse(model, name, df):\n",
        "  metrics = model.evaluate(input_fn = make_input_fn(df,1))\n",
        "  print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\n",
        "print_rmse(model, 'validation', df_valid)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-04-10T05:17:39Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-608\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-04-10-05:17:39\n",
            "INFO:tensorflow:Saving dict for global step 608: average_loss = 108.82821, global_step = 608, label/mean = 11.666427, loss = 12942.783, prediction/mean = 11.719884\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 608: taxi_trained/model.ckpt-608\n",
            "RMSE on validation dataset = 10.432075500488281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FRmRPQ5Nstvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is nowhere near our benchmark (RMSE of $6 or so on this data), but it serves to demonstrate what TensorFlow code looks like.\n",
        "\n",
        "Let's use this model for prediction."
      ]
    },
    {
      "metadata": {
        "id": "NjRsyfQr6tyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "d4cf6e87-d703-4002-dbc5-70f915f4ba80"
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "# Read saved model and use it for prediction\n",
        "model = tf.estimator.LinearRegressor(\n",
        "  feature_columns = make_feature_cols(), model_dir = outdir)\n",
        "\n",
        "preds_iter = model.predict(input_fn=make_input_fn(df_valid, 1))\n",
        "print([pred['predictions'][0] for pred in list(itertools.islice(preds_iter, 5))])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'taxi_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f01c4fb5978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-608\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "[11.67399, 11.679897, 11.736956, 11.677842, 11.678486]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AiYbqHcp7lnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This explains why the RMSE was so high -- the model essentially predicts the same amount for every trip. Would a more complex model help? Let's try using a deep neural network. The code to do this is quite straightforward as well."
      ]
    },
    {
      "metadata": {
        "id": "ilkBfkzY8QID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deep Neural Network regression"
      ]
    },
    {
      "metadata": {
        "id": "6ph6R-7Y8V60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2570
        },
        "outputId": "e8b03bcf-1fc8-4c44-a357-d5d06f5a534f"
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
        "model = tf.estimator.DNNRegressor(hidden_units=[32,8,2],\n",
        "  feature_columns = make_feature_cols(), model_dir = outdir)\n",
        "\n",
        "model.train(input_fn=make_input_fn(df_train, num_epochs = 100));\n",
        "print_rmse(model,'validation', df_valid)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'taxi_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f01c719d518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:loss = 24822.809, step = 1\n",
            "INFO:tensorflow:global_step/sec: 346.573\n",
            "INFO:tensorflow:loss = 32017.453, step = 101 (0.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 374.679\n",
            "INFO:tensorflow:loss = 24839.496, step = 201 (0.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 292.122\n",
            "INFO:tensorflow:loss = 37447.24, step = 301 (0.342 sec)\n",
            "INFO:tensorflow:global_step/sec: 397.22\n",
            "INFO:tensorflow:loss = 16582.984, step = 401 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 366.504\n",
            "INFO:tensorflow:loss = 27707.918, step = 501 (0.272 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.125\n",
            "INFO:tensorflow:loss = 13094.686, step = 601 (0.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 376.024\n",
            "INFO:tensorflow:loss = 24787.686, step = 701 (0.269 sec)\n",
            "INFO:tensorflow:global_step/sec: 392.306\n",
            "INFO:tensorflow:loss = 19860.457, step = 801 (0.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 388.125\n",
            "INFO:tensorflow:loss = 15805.012, step = 901 (0.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.121\n",
            "INFO:tensorflow:loss = 20665.195, step = 1001 (0.250 sec)\n",
            "INFO:tensorflow:global_step/sec: 398.694\n",
            "INFO:tensorflow:loss = 24448.734, step = 1101 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 401.174\n",
            "INFO:tensorflow:loss = 16566.29, step = 1201 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 393.955\n",
            "INFO:tensorflow:loss = 25160.605, step = 1301 (0.253 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.37\n",
            "INFO:tensorflow:loss = 15191.952, step = 1401 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.604\n",
            "INFO:tensorflow:loss = 21014.527, step = 1501 (0.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 352.916\n",
            "INFO:tensorflow:loss = 17035.297, step = 1601 (0.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 340.261\n",
            "INFO:tensorflow:loss = 15839.45, step = 1701 (0.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 340.45\n",
            "INFO:tensorflow:loss = 21595.129, step = 1801 (0.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 337.334\n",
            "INFO:tensorflow:loss = 14735.467, step = 1901 (0.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 336.629\n",
            "INFO:tensorflow:loss = 14511.712, step = 2001 (0.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 340.654\n",
            "INFO:tensorflow:loss = 12063.193, step = 2101 (0.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 341.648\n",
            "INFO:tensorflow:loss = 22578.232, step = 2201 (0.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 334.786\n",
            "INFO:tensorflow:loss = 11366.585, step = 2301 (0.302 sec)\n",
            "INFO:tensorflow:global_step/sec: 340.399\n",
            "INFO:tensorflow:loss = 13332.218, step = 2401 (0.291 sec)\n",
            "INFO:tensorflow:global_step/sec: 332.749\n",
            "INFO:tensorflow:loss = 16976.207, step = 2501 (0.300 sec)\n",
            "INFO:tensorflow:global_step/sec: 341.895\n",
            "INFO:tensorflow:loss = 11266.238, step = 2601 (0.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 334.546\n",
            "INFO:tensorflow:loss = 24751.262, step = 2701 (0.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 337.55\n",
            "INFO:tensorflow:loss = 9693.53, step = 2801 (0.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 334.058\n",
            "INFO:tensorflow:loss = 28259.977, step = 2901 (0.300 sec)\n",
            "INFO:tensorflow:global_step/sec: 334.91\n",
            "INFO:tensorflow:loss = 17227.082, step = 3001 (0.304 sec)\n",
            "INFO:tensorflow:global_step/sec: 338.129\n",
            "INFO:tensorflow:loss = 15672.736, step = 3101 (0.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 338.372\n",
            "INFO:tensorflow:loss = 10238.157, step = 3201 (0.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 334.886\n",
            "INFO:tensorflow:loss = 13216.687, step = 3301 (0.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 390.157\n",
            "INFO:tensorflow:loss = 14153.0, step = 3401 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.942\n",
            "INFO:tensorflow:loss = 14253.738, step = 3501 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.063\n",
            "INFO:tensorflow:loss = 9346.746, step = 3601 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 400.53\n",
            "INFO:tensorflow:loss = 17348.516, step = 3701 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 401.244\n",
            "INFO:tensorflow:loss = 12442.497, step = 3801 (0.250 sec)\n",
            "INFO:tensorflow:global_step/sec: 406.563\n",
            "INFO:tensorflow:loss = 22166.273, step = 3901 (0.245 sec)\n",
            "INFO:tensorflow:global_step/sec: 399.155\n",
            "INFO:tensorflow:loss = 9889.552, step = 4001 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 397.753\n",
            "INFO:tensorflow:loss = 8958.924, step = 4101 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 410.824\n",
            "INFO:tensorflow:loss = 6784.4883, step = 4201 (0.238 sec)\n",
            "INFO:tensorflow:global_step/sec: 386.85\n",
            "INFO:tensorflow:loss = 11325.344, step = 4301 (0.259 sec)\n",
            "INFO:tensorflow:global_step/sec: 369.094\n",
            "INFO:tensorflow:loss = 6656.9497, step = 4401 (0.271 sec)\n",
            "INFO:tensorflow:global_step/sec: 383.142\n",
            "INFO:tensorflow:loss = 21550.04, step = 4501 (0.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 391.205\n",
            "INFO:tensorflow:loss = 23999.34, step = 4601 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 396.054\n",
            "INFO:tensorflow:loss = 9267.019, step = 4701 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 392.365\n",
            "INFO:tensorflow:loss = 18578.79, step = 4801 (0.254 sec)\n",
            "INFO:tensorflow:global_step/sec: 398.952\n",
            "INFO:tensorflow:loss = 12695.679, step = 4901 (0.252 sec)\n",
            "INFO:tensorflow:global_step/sec: 392.95\n",
            "INFO:tensorflow:loss = 21367.215, step = 5001 (0.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 389.218\n",
            "INFO:tensorflow:loss = 20486.094, step = 5101 (0.256 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.26\n",
            "INFO:tensorflow:loss = 14649.896, step = 5201 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 404.324\n",
            "INFO:tensorflow:loss = 14472.187, step = 5301 (0.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 394.764\n",
            "INFO:tensorflow:loss = 15872.774, step = 5401 (0.257 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.356\n",
            "INFO:tensorflow:loss = 7508.4736, step = 5501 (0.245 sec)\n",
            "INFO:tensorflow:global_step/sec: 399.544\n",
            "INFO:tensorflow:loss = 15348.234, step = 5601 (0.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 381.723\n",
            "INFO:tensorflow:loss = 12034.142, step = 5701 (0.261 sec)\n",
            "INFO:tensorflow:global_step/sec: 403.04\n",
            "INFO:tensorflow:loss = 8148.1494, step = 5801 (0.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 385.125\n",
            "INFO:tensorflow:loss = 11294.242, step = 5901 (0.258 sec)\n",
            "INFO:tensorflow:global_step/sec: 402.297\n",
            "INFO:tensorflow:loss = 10477.611, step = 6001 (0.248 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6071 into taxi_trained/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1695.3501.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-04-10T06:31:23Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from taxi_trained/model.ckpt-6071\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2019-04-10-06:31:24\n",
            "INFO:tensorflow:Saving dict for global step 6071: average_loss = 134.89653, global_step = 6071, label/mean = 11.666427, loss = 16043.052, prediction/mean = 6.5559607\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 6071: taxi_trained/model.ckpt-6071\n",
            "RMSE on validation dataset = 11.614496231079102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bsiv6OZ_9ZEE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are not beating our benchmark with either model ... what's up? Well, we may be using TensorFlow for Machine Learning, but we are not yet using it well. That's what the rest of this course is about!\n",
        "\n",
        "\n",
        "But, for the record, let's say we had to choose between the two models. We'd choose the one with the lower validation error. Finally, we'd measure the RMSE on the test data with this chosen model."
      ]
    },
    {
      "metadata": {
        "id": "aQzY5h0c9pJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Benchmark dataset"
      ]
    },
    {
      "metadata": {
        "id": "oQ3ef_h29wT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1509
        },
        "outputId": "165ddd3d-3d22-418d-c49c-20e82ed67da6"
      },
      "cell_type": "code",
      "source": [
        "import datalab.bigquery as bq\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./bigquery_user.json\"\n",
        "\n",
        "def create_query(phase, every_n):\n",
        "  \"\"\"\n",
        "  phase: 1 = train 2 = valid\n",
        "  \"\"\"\n",
        "  base_query = \"\"\"\n",
        "#standardSQL\n",
        "SELECT\n",
        "  (tolls_amount + fare_amount) AS fare_amount,\n",
        "  GENERATE_UUID() AS key,\n",
        "  EXTRACT(DAYOFWEEK FROM pickup_datetime)*1.0 AS dayofweek,\n",
        "  EXTRACT(HOUR FROM pickup_datetime)*1.0 AS hourofday,\n",
        "  pickup_longitude AS pickuplon,\n",
        "  pickup_latitude AS pickuplat,\n",
        "  dropoff_longitude AS dropofflon,\n",
        "  dropoff_latitude AS dropofflat,\n",
        "  passenger_count*1.0 AS passengers\n",
        "FROM\n",
        "  `bigquery-public-data.new_york.tlc_yellow_trips_2016`\n",
        "WHERE\n",
        "  trip_distance > 0\n",
        "  AND fare_amount >= 2.5\n",
        "  AND pickup_longitude > -78\n",
        "  AND pickup_longitude < -70\n",
        "  AND dropoff_longitude > -78\n",
        "  AND dropoff_longitude < -70\n",
        "  AND pickup_latitude > 37\n",
        "  AND pickup_latitude < 45\n",
        "  AND dropoff_latitude > 37\n",
        "  AND dropoff_latitude < 45\n",
        "  AND passenger_count > 0\n",
        "  \"\"\"\n",
        "  \n",
        "  if every_n == None:\n",
        "    if phase < 2:\n",
        "      # Training\n",
        "      query = \"{0}AND MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))),4) < 2\".format(base_query)\n",
        "    else:\n",
        "      # Validation\n",
        "      query = \"{0}AND MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))),4) = {1}\".format(base_query, phase)\n",
        "  else:\n",
        "    query = \"{0}AND MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), {1} ) = {2}\".format(base_query, every_n, phase)\n",
        "    \n",
        "  return query\n",
        "\n",
        "\n",
        "\n",
        "query = create_query(2, 100000)\n",
        "print(query)\n",
        "df = bq.Query(query).to_dataframe()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "#standardSQL\n",
            "SELECT\n",
            "  (tolls_amount + fare_amount) AS fare_amount,\n",
            "  GENERATE_UUID() AS key,\n",
            "  EXTRACT(DAYOFWEEK FROM pickup_datetime)*1.0 AS dayofweek,\n",
            "  EXTRACT(HOUR FROM pickup_datetime)*1.0 AS hourofday,\n",
            "  pickup_longitude AS pickuplon,\n",
            "  pickup_latitude AS pickuplat,\n",
            "  dropoff_longitude AS dropofflon,\n",
            "  dropoff_latitude AS dropofflat,\n",
            "  passenger_count*1.0 AS passengers\n",
            "FROM\n",
            "  `bigquery-public-data.new_york.tlc_yellow_trips_2016`\n",
            "WHERE\n",
            "  trip_distance > 0\n",
            "  AND fare_amount >= 2.5\n",
            "  AND pickup_longitude > -78\n",
            "  AND pickup_longitude < -70\n",
            "  AND dropoff_longitude > -78\n",
            "  AND dropoff_longitude < -70\n",
            "  AND pickup_latitude > 37\n",
            "  AND pickup_latitude < 45\n",
            "  AND dropoff_latitude > 37\n",
            "  AND dropoff_latitude < 45\n",
            "  AND passenger_count > 0\n",
            "  AND MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 100000 ) = 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RequestException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRequestException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1ef29df7da19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, start_row, max_rows, use_cache, dialect, billing_tier)\u001b[0m\n\u001b[1;32m    320\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mPandas\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtable\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbilling_tier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbilling_tier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_row\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mresults\u001b[0;34m(self, use_cache, dialect, billing_tier)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \"\"\"\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbilling_tier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbilling_tier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, table_name, table_mode, use_cache, priority, allow_large_results, dialect, billing_tier)\u001b[0m\n\u001b[1;32m    525\u001b[0m     job = self.execute_async(table_name=table_name, table_mode=table_mode, use_cache=use_cache,\n\u001b[1;32m    526\u001b[0m                              \u001b[0mpriority\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpriority\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_large_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_large_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                              dialect=dialect, billing_tier=billing_tier)\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mexecute_async\u001b[0;34m(self, table_name, table_mode, use_cache, priority, allow_large_results, dialect, billing_tier)\u001b[0m\n\u001b[1;32m    480\u001b[0m                                                  billing_tier=billing_tier)\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'jobReference'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unexpected response from server'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mexecute_async\u001b[0;34m(self, table_name, table_mode, use_cache, priority, allow_large_results, dialect, billing_tier)\u001b[0m\n\u001b[1;32m    478\u001b[0m                                                  \u001b[0mtable_definitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_external_tables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                                                  \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                                                  billing_tier=billing_tier)\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datalab/bigquery/_api.py\u001b[0m in \u001b[0;36mjobs_insert_query\u001b[0;34m(self, sql, code, imports, table_name, append, overwrite, dry_run, use_cache, batch, allow_large_results, table_definitions, dialect, billing_tier)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mquery_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximumBillingTier'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilling_tier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatalab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_credentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mjobs_query_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/datalab/utils/_http.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(url, args, data, headers, method, credentials, raw_response, stats)\u001b[0m\n\u001b[1;32m    153\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRequestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to process HTTP response.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRequestException\u001b[0m: HTTP request failed: Query text specifies use_legacy_sql:false, while API options specify:true"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rb0BXL6nQSz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_rmse(model, 'benchmark', df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}