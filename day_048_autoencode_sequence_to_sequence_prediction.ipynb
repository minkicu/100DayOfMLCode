{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_048_autoencode_sequence_to_sequence_prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/100DayOfMLCode/blob/master/day_048_autoencode_sequence_to_sequence_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vqbgFmSHDr2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "fc7c46ab-586c-4404-a01e-2d56b8749cad"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "from numpy import argmax, array_equal\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from imgaug import augmenters\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "((x_train, y_train), (x_test, y_test)) = fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 4us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 3s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "faTIG0uAERs0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
        "\n",
        "\n",
        "Autoencoder Architecture\n",
        "The architecuture of this use case will contain an encoder to encode the source sequence and second to decode the encoded source sequence into the target sequence, called the decoder. First lets understand the internal working of LSTMs which will be used in this architecture.\n",
        "\n",
        "   - The Long Short-Term Memory, or LSTM, is a recurrent neural network that is comprised of internal gates.\n",
        "   - Unlike other recurrent neural networks, the network’s internal gates allow the model to be trained successfully using backpropagation through time, or BPTT, and avoid the vanishing gradients problem.\n",
        "   - We can define the number of LSTM memory units in the LSTM layer, Each unit or cell within the layer has an internal memory / cell state, often abbreviated as “c“, and outputs a hidden state, often abbreviated as “h“.\n",
        "   - By using Keras, we can access both output states of the LSTM layer as well as the current states of the LSTM layers.\n",
        "Lets now create an autoencoder architecutre for learning and producing sequences made up of LSTM layers. There are two components:\n",
        "\n",
        "   - An encoder architecture which takes a sequence as input and returns the current state of LSTM as the output\n",
        "   - A decoder architecture which takes the sequence and encoder LSTM states as input and returns the decoded output sequence\n",
        "   - We are saving and accessing hidden and memory states of LSTM so that we can use them while generating predictions on unseen data.\n",
        "Lets first of all, generate a sequence dataset containing random sequences of fixed lengths. We will create a function to generate random sequences.\n",
        "\n",
        "   - X1 repersents the input sequence containing random numbers\n",
        "   - X2 repersents the padded sequence which is used as the seed to reproduce the other elements of the sequence\n",
        "   - y repersents the target sequence or the actual sequence"
      ]
    },
    {
      "metadata": {
        "id": "dRvp574gEEfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6fc9e3cc-8fce-4781-a5eb-8d4b51b08994"
      },
      "cell_type": "code",
      "source": [
        "def dataset_preparation(n_in, n_out, n_unique, n_samples):\n",
        "    X1, X2, y = [], [], []\n",
        "    for _ in range(n_samples):\n",
        "        ## create random numbers sequence - input \n",
        "        inp_seq = [randint(1, n_unique-1) for _ in range(n_in)]\n",
        "        \n",
        "        ## create target sequence\n",
        "        target = inp_seq[:n_out]\n",
        "    \n",
        "        ## create padded sequence / seed sequence \n",
        "        target_seq = list(reversed(target))\n",
        "        seed_seq = [0] + target_seq[:-1]  \n",
        "        \n",
        "        # convert the elements to categorical using keras api\n",
        "        X1.append(to_categorical([inp_seq], num_classes=n_unique))\n",
        "        X2.append(to_categorical([seed_seq], num_classes=n_unique))\n",
        "        y.append(to_categorical([target_seq], num_classes=n_unique))\n",
        "    \n",
        "    # remove unnecessary dimention\n",
        "    X1 = np.squeeze(np.array(X1), axis=1) \n",
        "    X2 = np.squeeze(np.array(X2), axis=1) \n",
        "    y  = np.squeeze(np.array(y), axis=1) \n",
        "    return X1, X2, y\n",
        "\n",
        "samples = 100000\n",
        "features = 51\n",
        "inp_size = 6\n",
        "out_size = 3\n",
        "\n",
        "inputs, seeds, outputs = dataset_preparation(inp_size, out_size, features, samples)\n",
        "print(\"Shapes: \", inputs.shape, seeds.shape, outputs.shape)\n",
        "print (\"Here is first categorically encoded input sequence looks like: \", )\n",
        "inputs[0][0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shapes:  (100000, 6, 51) (100000, 3, 51) (100000, 3, 51)\n",
            "Here is first categorically encoded input sequence looks like: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "rGfqkr2xEzWL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, lets create the architecture of our model in Keras."
      ]
    },
    {
      "metadata": {
        "id": "PBRch8RCEwMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e8e6cab8-6241-472a-8bfc-07f723b1eee6"
      },
      "cell_type": "code",
      "source": [
        "def define_models(n_input, n_output):\n",
        "    ## define the encoder architecture \n",
        "    ## input : sequence \n",
        "    ## output : encoder states \n",
        "    encoder_inputs = Input(shape=(None, n_input))\n",
        "    encoder = LSTM(128, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    ## define the encoder-decoder architecture \n",
        "    ## input : a seed sequence \n",
        "    ## output : decoder states, decoded output \n",
        "    decoder_inputs = Input(shape=(None, n_output))\n",
        "    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(n_output, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    \n",
        "    ## define the decoder model\n",
        "    ## input : current states + encoded sequence\n",
        "    ## output : decoded sequence\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "    decoder_state_input_h = Input(shape=(128,))\n",
        "    decoder_state_input_c = Input(shape=(128,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "autoencoder, encoder_model, decoder_model = define_models(features, features)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "08ox8QKrE8fE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "69af77c3-8a30-4d1c-a1f5-056178d057de"
      },
      "cell_type": "code",
      "source": [
        "encoder_model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None, 51)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                [(None, 128), (None, 128) 92160     \n",
            "=================================================================\n",
            "Total params: 92,160\n",
            "Trainable params: 92,160\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8knzjbWIE9Io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "74abba09-33fd-45d9-fc95-67e02c342181"
      },
      "cell_type": "code",
      "source": [
        "decoder_model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, None, 51)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 128)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 128),  92160       input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 51)     6579        lstm_2[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 98,739\n",
            "Trainable params: 98,739\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cwYrRlBHFAn3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "a33b685a-290b-488a-85d2-45c8014bdc0a"
      },
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 51)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None, 51)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 128), (None, 92160       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 128),  92160       input_2[0][0]                    \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 51)     6579        lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 190,899\n",
            "Trainable params: 190,899\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-XiRjFRuFEgm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, lets train the autoencoder model using Adam optimizer and Categorical Cross Entropy loss function"
      ]
    },
    {
      "metadata": {
        "id": "0k7GJE76FFG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "d3090619-4c60-41d4-c68c-8404e269855a"
      },
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "autoencoder.fit([inputs, seeds], outputs, epochs=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/1\n",
            "100000/100000 [==============================] - 75s 748us/step - loss: 0.6504 - acc: 0.7915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd516873048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "MHXSXf_bFJ8y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets write a function to predict the sequence based on input sequence"
      ]
    },
    {
      "metadata": {
        "id": "nW5zY2-3FKa4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def reverse_onehot(encoded_seq):\n",
        "    return [argmax(vector) for vector in encoded_seq]\n",
        "\n",
        "def predict_sequence(encoder, decoder, sequence):\n",
        "    output = []\n",
        "    target_seq = np.array([0.0 for _ in range(features)])\n",
        "    target_seq = target_seq.reshape(1, 1, features)\n",
        "\n",
        "    current_state = encoder.predict(sequence)\n",
        "    for t in range(out_size):\n",
        "        pred, h, c = decoder.predict([target_seq] + current_state)\n",
        "        output.append(pred[0, 0, :])\n",
        "        current_state = [h, c]\n",
        "        target_seq = pred\n",
        "    return np.array(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYa0_wzNFPkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f99dd2d8-03b9-4be2-fa74-8814822c2aed"
      },
      "cell_type": "code",
      "source": [
        "for k in range(5):\n",
        "    X1, X2, y = dataset_preparation(inp_size, out_size, features, 1)\n",
        "    target = predict_sequence(encoder_model, decoder_model, X1)\n",
        "    print('\\nInput Sequence=%s SeedSequence=%s, PredictedSequence=%s' \n",
        "          % (reverse_onehot(X1[0]), reverse_onehot(y[0]), reverse_onehot(target)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input Sequence=[38, 41, 6, 16, 2, 38] SeedSequence=[6, 41, 38], PredictedSequence=[6, 41, 38]\n",
            "\n",
            "Input Sequence=[15, 46, 32, 33, 32, 30] SeedSequence=[32, 46, 15], PredictedSequence=[32, 46, 15]\n",
            "\n",
            "Input Sequence=[25, 28, 5, 49, 27, 1] SeedSequence=[5, 28, 25], PredictedSequence=[5, 28, 25]\n",
            "\n",
            "Input Sequence=[20, 33, 25, 12, 36, 35] SeedSequence=[25, 33, 20], PredictedSequence=[25, 33, 20]\n",
            "\n",
            "Input Sequence=[23, 6, 34, 18, 32, 36] SeedSequence=[34, 6, 23], PredictedSequence=[34, 6, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}