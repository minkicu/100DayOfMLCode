{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_073_cleaning_text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkrissada/100DayOfMLCode/blob/master/day_073_cleaning_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoLx73BN3_R8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_docs = [\"I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador...\",\n",
        "\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \\\"Jumbo\\\".\",\n",
        "\"This taffy is so good. It is very soft and chewy. The flavors are amazing. I would definitely recommend you buying it. Very satisfying!!\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXVsng-R5iPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6c236bb1-9e11-496a-b9a3-b108058561d9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ek5Xify3_SA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d6588eb5-a799-45bf-ab2b-47e07d06cf4f"
      },
      "source": [
        "# Tokenizing text into bags of words\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
        "print(tokenized_docs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', '...'], ['Product', 'arrived', 'labeled', 'as', 'Jumbo', 'Salted', 'Peanuts', '...', 'the', 'peanuts', 'were', 'actually', 'small', 'sized', 'unsalted', '.', 'Not', 'sure', 'if', 'this', 'was', 'an', 'error', 'or', 'if', 'the', 'vendor', 'intended', 'to', 'represent', 'the', 'product', 'as', '``', 'Jumbo', \"''\", '.'], ['This', 'taffy', 'is', 'so', 'good', '.', 'It', 'is', 'very', 'soft', 'and', 'chewy', '.', 'The', 'flavors', 'are', 'amazing', '.', 'I', 'would', 'definitely', 'recommend', 'you', 'buying', 'it', '.', 'Very', 'satisfying', '!', '!']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7COoEQQ3_SE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "489e4851-c932-4558-a847-7a47b109392f"
      },
      "source": [
        "# Removing punctuation\n",
        "import re\n",
        "import string\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "tokenized_docs_no_punctuation = []\n",
        "\n",
        "for review in tokenized_docs:\n",
        "    new_review = []\n",
        "    for token in review:\n",
        "        new_token = regex.sub(u'', token)\n",
        "        if not new_token == u'':\n",
        "            new_review.append(new_token)\n",
        "    \n",
        "    tokenized_docs_no_punctuation.append(new_review)\n",
        "    \n",
        "print(tokenized_docs_no_punctuation)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', 'My', 'Labrador'], ['Product', 'arrived', 'labeled', 'as', 'Jumbo', 'Salted', 'Peanuts', 'the', 'peanuts', 'were', 'actually', 'small', 'sized', 'unsalted', 'Not', 'sure', 'if', 'this', 'was', 'an', 'error', 'or', 'if', 'the', 'vendor', 'intended', 'to', 'represent', 'the', 'product', 'as', 'Jumbo'], ['This', 'taffy', 'is', 'so', 'good', 'It', 'is', 'very', 'soft', 'and', 'chewy', 'The', 'flavors', 'are', 'amazing', 'I', 'would', 'definitely', 'recommend', 'you', 'buying', 'it', 'Very', 'satisfying']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWX2xy5j3_SH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "845294c8-1d9e-461b-b7c3-d0218e19b17a"
      },
      "source": [
        "# Cleaning text of stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "tokenized_docs_no_stopwords = []\n",
        "\n",
        "for doc in tokenized_docs_no_punctuation:\n",
        "    new_term_vector = []\n",
        "    for word in doc:\n",
        "        if not word in stopwords.words('english'):\n",
        "            new_term_vector.append(word)\n",
        "    \n",
        "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
        "\n",
        "print(tokenized_docs_no_stopwords)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['I', 'bought', 'several', 'Vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'The', 'product', 'looks', 'like', 'stew', 'processed', 'meat', 'smells', 'better', 'My', 'Labrador'], ['Product', 'arrived', 'labeled', 'Jumbo', 'Salted', 'Peanuts', 'peanuts', 'actually', 'small', 'sized', 'unsalted', 'Not', 'sure', 'error', 'vendor', 'intended', 'represent', 'product', 'Jumbo'], ['This', 'taffy', 'good', 'It', 'soft', 'chewy', 'The', 'flavors', 'amazing', 'I', 'would', 'definitely', 'recommend', 'buying', 'Very', 'satisfying']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNi8mvCg3_SK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2b666acf-ba12-44bc-8a53-e674564f6b20"
      },
      "source": [
        "# Stemming and Lemmatizing\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "preprocessed_docs = []\n",
        "\n",
        "for doc in tokenized_docs_no_stopwords:\n",
        "    final_doc = []\n",
        "    for word in doc:\n",
        "        #final_doc.append(porter.stem(word))\n",
        "        final_doc.append(snowball.stem(word))\n",
        "        #final_doc.append(wordnet.lemmatize(word))\n",
        "    \n",
        "    preprocessed_docs.append(final_doc)\n",
        "\n",
        "print(preprocessed_docs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['i', 'bought', 'sever', 'vital', 'can', 'dog', 'food', 'product', 'found', 'good', 'qualiti', 'the', 'product', 'look', 'like', 'stew', 'process', 'meat', 'smell', 'better', 'my', 'labrador'], ['product', 'arriv', 'label', 'jumbo', 'salt', 'peanut', 'peanut', 'actual', 'small', 'size', 'unsalt', 'not', 'sure', 'error', 'vendor', 'intend', 'repres', 'product', 'jumbo'], ['this', 'taffi', 'good', 'it', 'soft', 'chewi', 'the', 'flavor', 'amaz', 'i', 'would', 'definit', 'recommend', 'buy', 'veri', 'satisfi']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}